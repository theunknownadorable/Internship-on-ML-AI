# -*- coding: utf-8 -*-
"""ICT_final_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CWx6_jmATEorELtfhKpl5ie9ohVc7SHC

#**Update_01**
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

data=pd.read_csv('/content/cardio_train.csv')

data.head()

data.shape

# converted no of days in 'age' column to no of years
data['age'] = data['age'] / 365

data.info()

data.isna().sum()

data.describe()

data.dtypes

# since the age column is in float converted it to int
data['age'] = data['age'].astype(int)
data['weight'] = data['weight'].astype(int)

data.head()

data.age.nunique()

data.height.nunique()

data.weight.nunique()

data.id.nunique()

gender = np.array(data['gender'].unique())

gender

# counts number of male in gender column
object_value_count = data['gender'].value_counts()['male']

# counts number of female in gender column
object_value_count2 = data['gender'].value_counts()['female']

# converted the values into percentage of 360 by dividing the number with total number of rows
# this is done to represent it in a pie chart
cardio = [(object_value_count/data.id.nunique())*360,(object_value_count2/data.id.nunique())*360]

cardio

# this pie chart shows percentage of males and females in gender column
plt.pie(cardio, labels=gender, autopct="%1.1f%%")
plt.show()

# number of males with cardio disease
num_males_with_1 = len(data[(data['gender'] == 'male') & (data['cardio'] == 1)])
num_males_with_1

# number of females with cardio disease
num_females_with_1 = len(data[(data['gender'] == 'female') & (data['cardio'] == 1)])
num_females_with_1

cardio_p=[len(data[(data['gender'] == 'male') & (data['cardio'] == 1)]),len(data[(data['gender'] == 'female') & (data['cardio'] == 1)])]
cardio_n=[len(data[(data['gender'] == 'male') & (data['cardio'] == 0)]),len(data[(data['gender'] == 'female') & (data['cardio'] == 0)])]

fig, (ax1, ax2) = plt.subplots(1, 2)
# Plot the pie chart of people with cardio disease in the first subplot
ax1.pie(cardio_p, labels=gender, autopct="%1.1f%%")
ax1.set_title('Females and males with cardio disease')

# Plot the pie chart of people without cardio disease in the second subplot
ax2.pie(cardio_n, labels=gender, autopct="%1.1f%%")
ax2.set_title('Females and males with no cardio disease')

# Adjust the spacing between the subplots
plt.subplots_adjust(wspace=2)

# Show the figure
plt.show()

# smokers and non smokers with cardio disease
smoke_p=[len(data[(data['smoke'] == 'yes') & (data['cardio'] == 1)]),len(data[(data['smoke'] == 'no') & (data['cardio'] == 1)])]
smoke_n=[len(data[(data['smoke'] == 'yes') & (data['cardio'] == 0)]),len(data[(data['smoke'] == 'no') & (data['cardio'] == 0)])]

smoker=['smoke','non-smoke']

fig, (ax1, ax2) = plt.subplots(1, 2)
# Plot the pie chart of people with cardio disease in the first subplot
ax1.pie(smoke_p, labels=smoker, autopct="%1.1f%%")
ax1.set_title('Smoker and non-smoker with cardio disease')

# Plot the pie chart of people without cardio disease in the second subplot
ax2.pie(smoke_n, labels=smoker, autopct="%1.1f%%")
ax2.set_title('Smoker and non-smoker with no cardio disease')

# Adjust the spacing between the subplots
plt.subplots_adjust(wspace=3)

# Show the figure
plt.show()

active=['active','non-active']
active_p=[len(data[(data['active'] == 'active') & (data['cardio'] == 1)]),len(data[(data['active'] == 'not active') & (data['cardio'] == 1)])]

plt.pie(active_p, labels=active, autopct="%1.1f%%")
plt.title('Active and non-active people with cardio')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['ap_hi'],data['ap_lo'])
plt.title('Plot of Diastolic vs Systolic Pressure',fontsize=16)
plt.xlabel('Diastolic')
plt.ylabel('Systolic')
plt.show()

plt.scatter(data.index,y=data['height'])

plt.scatter(data.index,y=data['weight'])

data[['age','height','weight','ap_lo','ap_hi','cardio']].corr()

# Select columns for the heatmap
heatmap_cols = ['age', 'height', 'weight', 'ap_lo', 'ap_hi', 'cardio']
heatmap_data = data[heatmap_cols]
# Create heatmap using seaborn
sns.heatmap(heatmap_data.corr(), cmap='coolwarm', annot=True)
plt.title('Correlation Heatmap')
plt.show()

# Filter data for males and females separately
male_data = data[data['gender'] == 'male']
female_data =data[data['gender'] == 'female']

# Create scatter plot for males and females over height
plt.scatter(male_data['height'], male_data['gender'], color='blue', alpha=0.5, label='Male')
plt.scatter(female_data['height'], female_data['gender'], color='red', alpha=0.5, label='Female')

# Set axis labels and legend
plt.xlabel('Height')
plt.ylabel('gender (1=Male, 2=Female)')
plt.legend()

# Display the plot
plt.show()

data.describe()

"""#**Update_02**

### **Label Encoding**
"""

from sklearn.preprocessing import LabelEncoder

# Create an instance of LabelEncoder
label_encoder = LabelEncoder()

# Fit the encoder on the gender column
label_encoder.fit(data['gender'])

# Transform the gender column with label encoding
data['gender'] = label_encoder.transform(data['gender'])
label_encoder.fit(data['cholesterol'])

# Transform the gender column with label encoding
data['cholesterol'] = label_encoder.transform(data['cholesterol'])

label_encoder.fit(data['gluc'])

# Transform the gender column with label encoding
data['gluc'] = label_encoder.transform(data['gluc'])

label_encoder.fit(data['active'])

# Transform the gender column with label encoding
data['active'] = label_encoder.transform(data['active'])

label_encoder.fit(data['smoke'])

# Transform the gender column with label encoding
data['smoke'] = label_encoder.transform(data['smoke'])

label_encoder.fit(data['alco'])

# Transform the gender column with label encoding
data['alco'] = label_encoder.transform(data['alco'])

data.head()

"""##**Outlier detection**

### **age**
"""

Q1 = np.percentile(data['age'],25,interpolation ='midpoint')
Q2 = np.percentile(data['age'],50,interpolation ='midpoint')
Q3 = np.percentile(data['age'],75,interpolation ='midpoint')
print('Q1 = ',Q1)
print('Q2 = ',Q2)
print('Q3 = ',Q3)

IQR = Q3-Q1
print('IQR = ',IQR)

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR
print('low_lim = ',low_lim)
print('up_lim = ',up_lim)

outlier=[]
for x in data['age']:
   if((x> up_lim) or (x < low_lim)):
       outlier.append(x)
print('Outliers = ',np.array(outlier))

data = data[(data['age'] >= low_lim) & (data['age'] <= up_lim)]

for i in ['age']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

"""### **height**"""

Q1 = np.percentile(data['height'],25,interpolation ='midpoint')
Q2 = np.percentile(data['height'],50,interpolation ='midpoint')
Q3 = np.percentile(data['height'],75,interpolation ='midpoint')
print('Q1 = ',Q1)
print('Q2 = ',Q2)
print('Q3 = ',Q3)

IQR = Q3-Q1
print('IQR = ',IQR)

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR
print('low_lim = ',low_lim)
print('up_lim = ',up_lim)

outlier=[]
for x in data['height']:
   if((x> up_lim) or (x < low_lim)):
       outlier.append(x)
print('Outliers = ',np.array(outlier))

data = data[(data['height'] >= low_lim) & (data['height'] <= up_lim)]

for i in ['height']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

"""### **weight**"""

Q1 = np.percentile(data['weight'],25,interpolation ='midpoint')
Q2 = np.percentile(data['weight'],50,interpolation ='midpoint')
Q3 = np.percentile(data['weight'],75,interpolation ='midpoint')
print('Q1 = ',Q1)
print('Q2 = ',Q2)
print('Q3 = ',Q3)

IQR = Q3-Q1
print('IQR = ',IQR)

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR
print('low_lim = ',low_lim)
print('up_lim = ',up_lim)

outlier=[]
for x in data['weight']:
   if((x> up_lim) or (x < low_lim)):
       outlier.append(x)
print('Outliers = ',np.array(outlier))

data = data[(data['weight'] >= low_lim) & (data['weight'] <= up_lim)]

for i in ['weight']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

"""###**ap_hi**"""

Q1 = np.percentile(data['ap_hi'],25,interpolation ='midpoint')
Q2 = np.percentile(data['ap_hi'],50,interpolation ='midpoint')
Q3 = np.percentile(data['ap_hi'],75,interpolation ='midpoint')
print('Q1 = ',Q1)
print('Q2 = ',Q2)
print('Q3 = ',Q3)

IQR = Q3-Q1
print('IQR = ',IQR)

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR
print('low_lim = ',low_lim)
print('up_lim = ',up_lim)

outlier=[]
for x in data['ap_hi']:
   if((x> up_lim) or (x < low_lim)):
       outlier.append(x)
print('Outliers = ',np.array(outlier))

data = data[(data['ap_hi'] >= low_lim) & (data['ap_hi'] <= up_lim)]

for i in ['ap_hi']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

"""###**ap_lo**"""

Q1 = np.percentile(data['ap_lo'],25,interpolation ='midpoint')
Q2 = np.percentile(data['ap_lo'],50,interpolation ='midpoint')
Q3 = np.percentile(data['ap_lo'],75,interpolation ='midpoint')
print('Q1 = ',Q1)
print('Q2 = ',Q2)
print('Q3 = ',Q3)

IQR = Q3-Q1
print('IQR = ',IQR)

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR
print('low_lim = ',low_lim)
print('up_lim = ',up_lim)

outlier=[]
for x in data['ap_lo']:
   if((x> up_lim) or (x < low_lim)):
       outlier.append(x)
print('Outliers = ',np.array(outlier))

data = data[(data['ap_lo'] >= low_lim) & (data['ap_lo'] <= up_lim)]

for i in ['ap_lo']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

"""**after outlier detection the shape is reduced to ▶▶**"""

data.shape

"""###**checking by plot values**"""

plt.figure(figsize=(6,4))
plt.scatter(data['ap_hi'],data['ap_lo'],s=5)
plt.title('Plot of Systolic BP vs Diastolic BP',fontsize=16)
plt.xlabel('ap_lo')
plt.ylabel('ap_hi')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['gender'],data['ap_hi'],s=10)
plt.title('Plot of Gender vs Systolic blood pressure',fontsize=16)  # 1 => male , 2 => female
plt.xlabel('gender')
plt.ylabel('ap_hi')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['gender'],data['ap_lo'],s=10)
plt.title('Plot of Gender vs Diastolic blood pressure',fontsize=16)
plt.xlabel('gender')
plt.ylabel('ap_lo')
plt.show()

plt.scatter(data.age,y=data['height'])
plt.title('Scatter plot of age with height')
plt.xlabel('age')
plt.ylabel('height')
plt.show()

plt.scatter(data.age,y=data['weight'])
plt.title('Scatter plot of age with weight')
plt.xlabel('age')
plt.ylabel('weight')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['height'],data['weight'],s=10)
plt.title('Plot of Height vs Weight',fontsize=16)
plt.xlabel('height')
plt.ylabel('weight')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['gender'],data['height'],s=10)
plt.title('Plot of Gender vs Height',fontsize=16)
plt.xlabel('gender')
plt.ylabel('ap_hi')
plt.show()

"""###**relation heatmap**"""

#  columns for the heatmap
heatmap_cols = ['age','gender','height', 'weight', 'ap_lo', 'ap_hi','cholesterol','gluc','smoke','alco','active','cardio']
heatmap_data = data[heatmap_cols]

# Create heatmap using seaborn
plt.figure(figsize=(15, 6)) 
sns.heatmap(heatmap_data.corr(), cmap='coolwarm', annot=True, cbar_kws={'shrink': 0.8})
plt.title('Correlation Heatmap')
plt.show()

"""###**applying scaling**"""

from sklearn.preprocessing import MinMaxScaler

# Create an instance of the MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

# Fit the scaler to the data and transform the features
scaled_data = scaler.fit_transform(data)

data.head(10)

#  columns for the heatmap
heatmap_cols = ['age','gender','height', 'weight', 'ap_lo', 'ap_hi','cholesterol','gluc','smoke','alco','active','cardio']
heatmap_data = data[heatmap_cols]

# Create heatmap using seaborn
plt.figure(figsize=(15, 6)) 
sns.heatmap(heatmap_data.corr(), cmap='coolwarm', annot=True, cbar_kws={'shrink': 0.8})
plt.title('Correlation Heatmap')
plt.show()

id=data['id']

id

data=data.drop(['id'],axis=1)

data.head()

data.info()

"""##**Splitting the data**

###**Linear regression model**
"""

X=data.drop('cardio',axis=1)
y=data['cardio']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
model = lr.fit(X_train, y_train)
prediction = model.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score
print('Mean squared error is:',mean_squared_error(y_test,prediction))
print('R squared value is:',r2_score(y_test,prediction))

#from sklearn.metrics import accuracy_score,confusion_matrix
#print('Accuracy is:',accuracy_score(y_test,prediction))
#print(confusion_matrix(y_test,prediction))

"""###**Decision tree model**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)

from sklearn.tree import DecisionTreeClassifier
dt_clf = DecisionTreeClassifier()
model_1 = dt_clf.fit(X_train,y_train)
prediction_1 = model_1.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score
print('Mean squared error is:',mean_squared_error(y_test,prediction_1))
print('R squared value is:',r2_score(y_test,prediction_1))

from sklearn.metrics import accuracy_score,confusion_matrix
print('Accuracy is:',accuracy_score(y_test,prediction_1))
print(confusion_matrix(y_test,prediction_1))

"""###**Random forest Classifier model**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)

from sklearn.ensemble import RandomForestClassifier
rf_clf = RandomForestClassifier()
model_2 = rf_clf.fit(X_train,y_train)
prediction_2 = model_2.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score
print('Mean squared error is:',mean_squared_error(y_test,prediction_2))
print('R squared value is:',r2_score(y_test,prediction_2))

from sklearn.metrics import accuracy_score,confusion_matrix
print('Accuracy is:',accuracy_score(y_test,prediction_2))
print(confusion_matrix(y_test,prediction_2))

"""###**Random forest Regression model**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)

from sklearn.ensemble import RandomForestRegressor
rf_clf = RandomForestRegressor()
model_3 = rf_clf.fit(X_train,y_train)
prediction_3 = model_3.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score
print('Mean squared error is:',mean_squared_error(y_test,prediction_3))
print('R squared value is:',r2_score(y_test,prediction_3))

"""###**Logistic regression model**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)

from sklearn.linear_model import LogisticRegression
logic_m=LogisticRegression()
model_4 = logic_m.fit(X_train,y_train)
prediction_4 = model_4.predict(X_test)

from sklearn.metrics import accuracy_score
print("Accuracy is : ",accuracy_score(y_test,prediction_4))

"""###**K Neighbours**"""

#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)

from sklearn.neighbors import KNeighborsClassifier
#from sklearn.model_selection import cross_val_score

# Define a range of k values to test
#k_values = range(1, 50)  # Test k values from 1 to 50                  #44

# Create an empty list to store the cross-validation scores
#cv_scores = []

#for k in k_values:
    #knn = KNeighborsClassifier(n_neighbors=k)
    #scores = cross_val_score(knn, X_train, y_train, cv=5)  # Perform 5-fold cross-validation
    #cv_scores.append(np.mean(scores))

# Print the cross-validation scores for each k value
#for k, score in zip(k_values, cv_scores):
    #print("K =", k, "  Cross-Validation Score =", score)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)

k = 44  # Set the number of neighbors
knn = KNeighborsClassifier(n_neighbors=k)
model_5 = knn.fit(X_train, y_train)
prediction_5 = model_5.predict(X_test)

accuracy = accuracy_score(y_test, prediction_5)
print('Accuracy:', accuracy)

print(confusion_matrix(y_test,prediction_5))

"""###**Support vector machines**"""

#X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.2)

#from sklearn.svm import SVC
#svmclf = SVC(kernel='linear')
#model_6 = svmclf.fit(X_train,y_train)
#prediction_6 = model_6.predict(X_test)

#from sklearn.metrics import mean_squared_error, r2_score
#print('Mean squared error is:',mean_squared_error(y_test,prediction_6))
#print('R squared value is:',r2_score(y_test,prediction_6))

#from sklearn.metrics import accuracy_score,confusion_matrix
#print('Accuracy is:',accuracy_score(y_test,prediction_6))
#print(confusion_matrix(y_test,prediction_6))

"""###**XG Boost algorithm**"""

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

"""# **Update_03**

## **Hyperparamter tuning**

### **Using Randomized CV**
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.model_selection import RandomizedSearchCV

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10],
          'min_samples_split': [2, 4, 6]
}

# Create a random forest classifier
rf = RandomForestClassifier()

# Perform random search cross-validation
random_search = RandomizedSearchCV(rf, param_grid, cv=5, n_iter=10)
random_search.fit(X_train, y_train)

# Print the best hyperparameters and the corresponding score
print("Random Search Best Hyperparameters: ", random_search.best_params_)
print("Random Search Best Score: ", random_search.best_score_)

"""### **Using GridSearch**"""

from sklearn.model_selection import GridSearchCV

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for hyperparameter tuning
param_grid = {
  'n_estimators': [50, 100, 200],
  'max_depth': [None, 5, 10],
  'min_samples_split': [2, 4, 6]
}

# Create a random forest classifier
rf = RandomForestClassifier()

# Perform grid search cross-validation
grid_search = GridSearchCV(rf, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and the corresponding score
print("Grid Search Best Hyperparameters: ", grid_search.best_params_)
print("Grid Search Best Score: ", grid_search.best_score_)

import pickle

# Make pickle file of our model
pickle.dump(xgb_model, open("model.pkl", "wb"))